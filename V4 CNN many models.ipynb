{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15699697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import mne_nirs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import pywt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c630f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_win = 30\n",
    "sc_factor = 1\n",
    "sample_rate = 7.81\n",
    "signal_rate = 7.81\n",
    "signal_length = time_win * signal_rate\n",
    "norm_base = 16\n",
    "print(signal_length)\n",
    "\n",
    "resting_state_path = \"/Users/kostasdemiris/Downloads/snirf/\"\n",
    "\n",
    "'''\n",
    "Blanco B, Molnar M, Carreiras M, Caballero-Gaudes C. \n",
    "Open access dataset of task-free hemodynamic activity in 4-month-old infants during sleep using fNIRS. \n",
    "Sci Data. 2022 Mar 25;9(1):102. doi: 10.1038/s41597-022-01210-y. PMID: 35338168; PMCID: PMC8956728.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the base MNE dataset\n",
    "fnirs_folder = mne.datasets.fnirs_motor.data_path()\n",
    "fnirs_cw_amplitude_dir = fnirs_folder / \"Participant-1\"\n",
    "raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir, verbose=True)\n",
    "raw_intensity.load_data()\n",
    "\n",
    "\n",
    "\n",
    "raw_intensity.annotations.set_durations(time_win)\n",
    "raw_intensity.annotations.rename(\n",
    "    {\"1.0\": \"Control\", \"2.0\": \"Tapping/Left\", \"3.0\": \"Tapping/Right\"}\n",
    ")\n",
    "unwanted = np.nonzero(raw_intensity.annotations.description == \"15.0\")\n",
    "raw_intensity.annotations.delete(unwanted)\n",
    "# Apparently channel 15 is unrelated to the motor activation experiment,\n",
    "# it was used to signal something unrelated and so will be ignored\n",
    "\n",
    "# Removes short channels\n",
    "picks = mne.pick_types(raw_intensity.info, meg = False, fnirs = True)\n",
    "dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "    raw_intensity.info, picks = picks)\n",
    "raw_intensity.pick(picks[dists > 0.01])\n",
    "\n",
    "\n",
    "\n",
    "# Converting to optical density based on readings, then to haemo readings\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=6)\n",
    "\n",
    "# We filter them to remove the frequencies associated with cardiac activity (unrelated)\n",
    "raw_haemo_unfiltered = raw_haemo.copy()\n",
    "# raw_haemo.filter(0.05, 0.7, h_trans_bandwidth = 0.2, l_trans_bandwidth = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "# Creates epochs for each occurence of an event (Left tap, right tap, control)\n",
    "reject_criteria = dict(hbo=80e-6)\n",
    "tmin, tmax = -1, time_win-1\n",
    "events, event_dict = mne.events_from_annotations(raw_haemo)\n",
    "\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw_haemo,\n",
    "    events,\n",
    "    event_id=event_dict,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    reject=reject_criteria,\n",
    "    reject_by_annotation=True,\n",
    "    proj=True,\n",
    "    baseline=(None, 0),\n",
    "    preload=True,\n",
    "    detrend=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "isolated_event_epochs = epochs[\"Tapping\"]\n",
    "control_event_set = epochs[\"Control\"]\n",
    "isol_dhrf_data = isolated_event_epochs.get_data(copy=True)\n",
    "isol_cont_data = control_event_set.get_data(copy=True)\n",
    "avg_haemo_func_tapping = isolated_event_epochs.average()\n",
    "std_dev = np.std(avg_haemo_func_tapping.data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30144c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = 7.81\n",
    "amp = .4\n",
    "\n",
    "raw = mne_nirs.simulation.simulate_nirs_raw(\n",
    "    sfreq=sfreq, sig_dur=60 * 5, amplitude=amp, isi_min=30., isi_max=60.)\n",
    "\n",
    "print(np.shape(raw.get_data()))\n",
    "experimental_overlay = isol_cont_data.copy()\n",
    "associated_hrfs = []\n",
    "print(np.shape(experimental_overlay))\n",
    "index = 0\n",
    "while index < len(experimental_overlay):\n",
    "    hrf_tape = mne_nirs.simulation.simulate_nirs_raw(\n",
    "        sfreq=sfreq, sig_dur=60 * 5, amplitude=amp, isi_min=30., isi_max=60.)\n",
    "    hrf_data = hrf_tape.get_data()[0]\n",
    "    generated_hrf, event_id = mne.events_from_annotations(hrf_tape)\n",
    "    for starting_point in generated_hrf:\n",
    "        if index < len(experimental_overlay) and starting_point[0] + signal_length < len(hrf_data):\n",
    "            experimental_overlay[index][0] += hrf_data[starting_point[0]: starting_point[0] + len(experimental_overlay[index][0])]\n",
    "            associated_hrfs.append(hrf_data[starting_point[0]: starting_point[0] + len(experimental_overlay[index][0])])\n",
    "            index += 1\n",
    "print(len(experimental_overlay), \"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snirf_data(folder_path, ID=0):\n",
    "    raw_data_array = []\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.snirf'):\n",
    "\n",
    "                snirf_file_path = os.path.join(folder_path, file_name)\n",
    "                raw_object = mne.io.read_raw_snirf(snirf_file_path, verbose=False)\n",
    "                \n",
    "                annotation = mne.Annotations(onset=[0], duration=[raw_object.times[-1]  - raw_object.times[0]], description=[f\"Subject ID: {ID}\"])\n",
    "                # Onset starts at 0, the start, and duration is until the last sample (so covers the whole thing)\n",
    "                ID += 1\n",
    "\n",
    "                raw_object.set_annotations(annotation)\n",
    "                raw_data_array.append(raw_object)\n",
    "    \n",
    "    return raw_data_array\n",
    "\n",
    "\n",
    "def get_time_series_from_snirf_data(snirf_file_path):\n",
    "    raw_object = mne.io.read_raw_snirf(snirf_file_path, verbose=False)\n",
    "    raw_object.load_data()\n",
    "    \n",
    "    # Removes short channels. They're useful for my single channel denoising program since they can't pick up hrf.\n",
    "    raw_picks = mne.pick_types(raw_object.info, meg = False, fnirs = True)\n",
    "    channel_distances = mne.preprocessing.nirs.source_detector_distances(raw_object.info, picks = raw_picks)\n",
    "    raw_object.pick(raw_picks[channel_distances > 0.01])\n",
    "\n",
    "    optical_density = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "    haemo_recording = mne.preprocessing.nirs.beer_lambert_law(optical_density, ppf=6)\n",
    "    haemo_data = haemo_recording.get_data()\n",
    "    \n",
    "    return haemo_data\n",
    "    \n",
    "\n",
    "time_state_data = np.array(get_time_series_from_snirf_data(\"/Users/kostasdemiris/Downloads/snirf/RS4_SL_4219.snirf\"))\n",
    "\n",
    "displays.plot_pChannel(time_state_data[0], np.arange(0, len(time_state_data[0]-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class display:\n",
    "    def __init__(self, time_sig):\n",
    "        self.time_signal = time_sig\n",
    "    \n",
    "    def display_gray_arr(self, img_arr):\n",
    "        plt.imshow(img_arr, cmap='gray')\n",
    "        plt.title(\"Wavelet As Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_pChannel(self, points, time_series):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(time_series, points)\n",
    "        plt.title(\"channel\")\n",
    "        plt.xlabel(\"Time [s]\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "    def plot_multiChannel(self, channel_arr, time_series):\n",
    "        fig, ax = plt.subplots()\n",
    "        for i, array in enumerate(channel_arr):\n",
    "            ax.plot(time_series, channel_arr[i], label=\"Channel {i+1}\")\n",
    "        ax.set_xlabel('Time: in seconds')\n",
    "        ax.set_ylabel('Amplitude ')\n",
    "        ax.set_title('Multiple Channels plotted on one image')\n",
    "    \n",
    "    def plot_scatter_data(self, data_points, central=0.5):\n",
    "        # Useful for power spectra, not currently used but could be\n",
    "        plt.scatter([i for i in range(len(data_points))], data_points, color='white', alpha=0.5)\n",
    "\n",
    "        for i in range(len(data_points)):\n",
    "            plt.plot([i, i], [data_points[i], central], color='red', alpha=0.5)\n",
    "\n",
    "        plt.axhline(y=central, color='green', linestyle='--')\n",
    "\n",
    "        plt.title('Scatter plot')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_comparision_data(self, original_data, reconstructed_data, time_signal):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(time_signal, original_data, label='Original')\n",
    "        plt.plot(time_signal, reconstructed_data, label='Reconstructed')\n",
    "        plt.title('Comparision')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "    def plot_wavedec_spect(self, coeffs):\n",
    "        coefficients_array = []\n",
    "        max_length = max(len(i) for i in coeffs)\n",
    "        \n",
    "        for level, row in enumerate(coeffs):\n",
    "            stretched_row = np.repeat(row, np.ceil(max_length / len(row)).astype(int))\n",
    "            coefficients_array.append(stretched_row[:max_length]) # Again, this is just for display, not actually used\n",
    "            \n",
    "            \n",
    "        coeffs_matrix = np.vstack(coefficients_array)\n",
    "        norm_coeff_matrix = np.repeat(normalise(coeffs_matrix), 3, axis=0)\n",
    "        \n",
    "        print(f\"shape is {np.shape(norm_coeff_matrix)}\")\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(norm_coeff_matrix, cmap='brg')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.ylabel('Depth')\n",
    "        plt.xlabel('Time')\n",
    "        plt.title('WAVEDEC DISPLAY SPECTOGRAM')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_wavedec_layer(self, coeffs, start_time=-1, stop_time=time_win-1):\n",
    "        for depth, row in enumerate(coeffs):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            time_row = np.linspace(start_time, stop_time, num=len(row))\n",
    "            plt.plot(time_row, row)\n",
    "            plt.title(f\"wavelet level {depth + 1}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.show()\n",
    "            \n",
    "    def plot_comp_wavedec_layer(self, coeffs, recoefs, start_time=-1, stop_time=time_win-1):\n",
    "        for depth, row in enumerate(coeffs):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            time_row = np.linspace(start_time, stop_time, num=len(row))\n",
    "            plt.plot(time_row, row, label='Original')\n",
    "            plt.plot(time_row, recoefs[depth], label='Reconstructed')\n",
    "            plt.title(f\"wavelet level {depth + 1}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "class Normalisations:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def z_normalisation(self, data, d_mean, d_std):\n",
    "        return (data - d_mean) / d_std\n",
    "    \n",
    "    def inv_z_normalisation(self, data, d_mean, d_std):\n",
    "        return (data * d_std) + d_mean\n",
    "    \n",
    "class Conversions:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cwt(self, complex_signal, wavelet='cmor', sample_rate=sample_rate):\n",
    "        # Using this cmor wavelet for the cwt, it's the complex morlet wavelet \n",
    "        scales = np.arange(1, 129)  \n",
    "        coefficients, frequencies = pywt.cwt(complex_signal, scales, wavelet, sampling_period=1/sample_rate)\n",
    "        return coefficients, frequencies\n",
    "\n",
    "    \n",
    "    def dwt(self, data, wavelet='db4'):\n",
    "        # db4 is chosen as the default basis function because it looks like the canonical hrf (kinda).\n",
    "        coefficients, frequencies = pywt.dwt(data, wavelet)\n",
    "        return coefficients, frequencies\n",
    "    \n",
    "    def wavedec(self, data, wavelet='sym4'):\n",
    "        max_depth = pywt.dwt_max_level(len(data), wavelet)\n",
    "        coeffiecients = pywt.wavedec(data, wavelet, level=max_depth)\n",
    "        return coeffiecients\n",
    "    \n",
    "    def waverec(self, coefficients, wavelet='sym4'):\n",
    "        recovered = pywt.waverec(coefficients, wavelet)\n",
    "        return recovered\n",
    "    \n",
    "    def inv_dwt(self, coeffs, freqs, wavelet='sym4'):\n",
    "        return pywt.idwt(coeffs, freqs, wavelet)\n",
    "\n",
    "class Evaluations:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def root_signal_power(self, signal):\n",
    "        square_sum = np.sum(np.power(signal, 2)) \n",
    "        root_mean = (square_sum / len(signal)) ** 0.5\n",
    "        return root_mean\n",
    "        \n",
    "    def signal_to_noise_ratio(self, signal, noise):\n",
    "        # Not exactly signal to noise. I'm using root of signal power because it was better for my purposes i guess\n",
    "        signal_power = self.root_signal_power(signal)\n",
    "        noise_power = self.root_signal_power(noise)\n",
    "        return signal_power / signal_noise\n",
    "    \n",
    "    def contrast_to_noise(self, signal, noise):\n",
    "        signal_power = self.root_signal_power(signal)\n",
    "        noise_power = self.root_signal_power(noise)\n",
    "        std_noise = np.std(noise)\n",
    "        return np.abs(signal_power - noise_power) / std_noise\n",
    "    \n",
    "    def root_mean_square_error(self, recon_signal, target_signal):\n",
    "        mean_error = np.mean((target_signal - recon_signal) ** 2)\n",
    "        return (mean_error ** 0.5)\n",
    "    \n",
    "    def peak_signal_to_noise_ratio(self, signal, desired_signal, max_i = 1):\n",
    "        # Maximum possible intensity (max_i) is assumed to be 1 due to normalisation\n",
    "        return (20 * np.log(10, max_i)) - (10 * np.log(10, self.root_mean_square_error(signal, desired_signal))) \n",
    "    \n",
    "class Generations:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def gen_frequency(self, frequency, amplitude, sample_rate, point_number, starting_offset=0):\n",
    "        f_timeseries, step_interval = [], 1 / (sample_rate * frequency)\n",
    "        for time in range(point_number):\n",
    "            f_timeseries.append(amplitude * math.sin(time * step_interval))\n",
    "        return np.array(f_timeseries)\n",
    "    \n",
    "    def gen_vari_frequency(self, frequency, f_variability, amplitude, a_variability, sample_rate, point_number, starting_offset=0):\n",
    "        f_timeseries, step_interval = [], 1 / (sample_rate * frequency)\n",
    "        target_frequency = np.random.normal(frequency, f_variability)\n",
    "        target_amplitude = np.random.normal(amplitude, a_variability)\n",
    "        for time in range(point_number):\n",
    "            if time % 100 == 0:\n",
    "                target_frequency = np.random.normal(frequency, f_variability)\n",
    "                target_amplitude = np.random.normal(amplitude, a_variability)\n",
    "                target_frequency = max(target_frequency, 1e-6)\n",
    "                target_amplitude = max(target_amplitude, 1e-10)\n",
    "            frequency += 0.1 * np.log(target_frequency) - np.log(frequency)\n",
    "            amplitude += 0.01 * amplitude * (np.log(target_amplitude) - np.log(amplitude))\n",
    "            step_interval =1 / (sample_rate * frequency)\n",
    "            value = (amplitude * math.sin(time * step_interval * frequency))\n",
    "            if np.isnan(value):\n",
    "                print(f\"amp {amplitude}, targ amp {target_amplitude}, freq {frequency}, time {time}, step_int {step_interval}\")\n",
    "\n",
    "            f_timeseries.append(value)\n",
    "        return np.array(f_timeseries)\n",
    "    \n",
    "    def gen_motion_artifact(self, signal_length, sample_rate = sample_rate, peak_amp_range=(0.25e-6, 1e-6), time_to_peak = 7.0, random_scale = (0.5, 1.5)):\n",
    "        peak_amp_range = np.random.uniform(peak_amp_range[0], peak_amp_range[1])\n",
    "        random_scale = np.random.uniform(random_scale[0], random_scale[1])\n",
    "        time_to_peak = time_to_peak + (random.uniform(-50, 50) * (1/sample_rate))\n",
    "        \n",
    "        motion_artifact = np.array([(peak_amp_range * np.exp(- np.absolute((t / sample_rate) - time_to_peak) / random_scale)) for t in range(signal_length)])\n",
    "        return motion_artifact\n",
    "    \n",
    "    # -------------- Individual Component Generation Section -----------\n",
    "    \n",
    "    def gen_white_noise(self, point_num):\n",
    "        return np.random.random_sample(size=(point_num,)) - 0.5\n",
    "    \n",
    "    def gen_nirs_data(self, sample_rate, signal_length, amplitude, min_nirs_interval=15.0, max_nirs_interval=45.0):\n",
    "        return mne_nirs.simulation.simulate_nirs_raw(sfreq=sample_rate, sig_dur=signal_length, amplitude=amplitude, isi_min=min_nirs_interval, isi_max=max_nirs_interval)\n",
    "    \n",
    "    def gen_heart_signal(self, sample_rate, point_num, frequency=0.8, variability=0.03, amplitude=1e-7):\n",
    "        #heart_sig = self.gen_frequency(frequency, amplitude, sample_rate, point_num, random.randint(0, 100))\n",
    "        heart_sig = self.gen_vari_frequency(frequency, variability, amplitude, amplitude/5, sample_rate, point_num, starting_offset=random.randint(0, 100))\n",
    "        return heart_sig\n",
    "    \n",
    "    def gen_mayer_signal(self, sample_rate, point_num, frequency=0.1, amplitude=5e-8):\n",
    "        mayer_sig = self.gen_frequency(frequency, amplitude, sample_rate, point_num, random.randint(0, 100))\n",
    "        return mayer_sig\n",
    "    \n",
    "    def gen_respi_signal(self, sample_rate, point_num, frequency=0.333, amplitude=1e-7):\n",
    "#         respi_sig = self.gen_frequency(frequency, amplitude, sample_rate, point_num, random.randint(0, 100))\n",
    "        respi_sig = self.gen_vari_frequency(frequency, frequency/25, amplitude, amplitude/5, sample_rate, point_num, starting_offset=random.randint(0, 100))\n",
    "        return respi_sig\n",
    "    \n",
    "    def gen_resting_state_data(self, sample_rate, point_num):\n",
    "        heart_signal = self.gen_heart_signal(sample_rate, point_num)\n",
    "        mayer_signal = self.gen_mayer_signal(sample_rate, point_num)\n",
    "        respi_signal = self.gen_respi_signal(sample_rate, point_num)\n",
    "#         if np.any(np.isnan(heart_signal)) or np.any(np.isnan(mayer_signal)) or np.any(np.isnan(respi_signal)):\n",
    "#             print(f\"heart {heart_signal}, mayer {mayer_signal}, respi {respi_signal}\")\n",
    "        return heart_signal + mayer_signal + respi_signal\n",
    "    \n",
    "    def generate_basic_test_data(self, data_length, sample_length=np.ceil(signal_length).astype(int)):\n",
    "        index = 0\n",
    "        test_size, hrf_number = data_length, data_length\n",
    "        hrf_set = np.zeros((test_size, sample_length))\n",
    "\n",
    "        while index < hrf_number:\n",
    "            print(f\"Index is currently {index}\")\n",
    "            hrf_tape = mne_nirs.simulation.simulate_nirs_raw(\n",
    "                    sfreq=sfreq, sig_dur=60 * 50, amplitude=amp, isi_min=30., isi_max=60., stim_dur=random.uniform(3., 6.))\n",
    "            hrf_data = hrf_tape.get_data()[0]\n",
    "            generated_hrf, event_id = mne.events_from_annotations(hrf_tape)\n",
    "\n",
    "            for start in generated_hrf:\n",
    "                if index < hrf_number and start[0] + sample_length < len(hrf_data):\n",
    "                    hrf_set[index] = (hrf_data[start[0]: start[0] + sample_length])\n",
    "                    index += 1\n",
    "        np.random.shuffle(hrf_set) # Random distribution of hrf and \"quiet\" signals\n",
    "\n",
    "        overlay_data = np.array([\n",
    "            np.roll(generator.gen_resting_state_data(sample_rate, sample_length)\n",
    "            + (generator.gen_white_noise(sample_length) * 1e-7),\n",
    "                   random.randint(0, 100))\n",
    "            for data_point in range(len(hrf_set))\n",
    "        ])\n",
    "\n",
    "\n",
    "        extract_data = np.zeros((test_size, sample_length))\n",
    "        for i in range(len(extract_data)):\n",
    "            shift_index = random.randint(0, (sample_length)//5)\n",
    "            extract_data[i] = np.concatenate((np.full(shift_index, 1e-15), (hrf_set[i] * np.random.uniform(low=0.6, high=1.4))[:sample_length-shift_index]))\n",
    "#             extract_data[i] =  hrf_set[i] * np.random.uniform(low=0.6, high=1.4)\n",
    "        \n",
    "        return extract_data, overlay_data\n",
    "    \n",
    "    # ---------------------- End of Individual Component Section -------------------\n",
    "    \n",
    "    \n",
    "# Data Generation models\n",
    "class Auto_Regressive_Model():\n",
    "    # Uses the direct relation between the previous k (order) values to forecast the subsequent value in a time series\n",
    "    # Suitable for when the ACF plot has a slowly decreasing tail, with the highest significant PACF value being order\n",
    "    def __init__(self, order):\n",
    "        self.k = order\n",
    "        self.model = LinearRegression()\n",
    "        self.std = None\n",
    "    \n",
    "    def generate_training_data(self, data):\n",
    "        # returns the inp. data, an array of k long sequences, and outp. data, array of the respective k+i+1'th values\n",
    "        n = len(data)\n",
    "                \n",
    "        x_data = np.reshape(data[:n - self.k], (-1, 1))\n",
    "        for i in range(1, self.k):\n",
    "            x_data = np.hstack((x_data, np.reshape(data[i: n - self.k + i], (-1, 1))))\n",
    "\n",
    "        y_data = data[self.k:]\n",
    "        return np.array(x_data), np.array(y_data)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.std = np.std(data)\n",
    "        training_x, training_y = self.generate_training_data(data)\n",
    "        self.model.fit(training_x, training_y)\n",
    "    \n",
    "    def predict(self, data, steps, mc_depth, iterative_averaging=False):\n",
    "        # Steps refers the number of points that it will return in the future, and mc_depth is the number of monte \n",
    "        # carlo simulations that we'll average to return the data\n",
    "        assert(mc_depth > 0 and steps > 0)\n",
    "        \n",
    "        inputted, output = np.array(data), []\n",
    "        tape = inputted[-self.k:]\n",
    "        if iterative_averaging:\n",
    "            # This approach predicts the next value r (mc_depth) times, then averages that and uses it as the subq. input\n",
    "            # reduces compounding error from consecutive inaccurate predictions, but is more comp. expensive and still \n",
    "            # can have bias if it accumalates over many predictions.\n",
    "            for step in range(steps):\n",
    "                temp_next = 0\n",
    "                \n",
    "                for sim in range(mc_depth):\n",
    "                    temp_next += (self.model.predict(np.reshape(tape, (1, -1))).item() + np.random.normal(scale = self.std))\n",
    "                    # we add an error term to prediction to conser the unpredictability and also for some non-determinism\n",
    "                \n",
    "                tape = np.roll(tape, -1)\n",
    "                next_point = temp_next / mc_depth\n",
    "                tape[-1] = next_point\n",
    "                output.append(next_point)\n",
    "            \n",
    "            output = np.array(output)\n",
    "            \n",
    "        else:\n",
    "            # averaging r (mc_depth) different simulations of k (order) points\n",
    "            \n",
    "            output_acc = np.array([])\n",
    "            # Accumulates all of the simulations\n",
    "            \n",
    "            for sim in range(mc_depth):\n",
    "                tape = inputted[-self.k:]\n",
    "                temp_output = []\n",
    "                \n",
    "                for step in range(steps):\n",
    "                    temp_next = self.model.predict(np.reshape(tape, (1, -1))).item() + np.random.normal(scale = self.std)\n",
    "                    temp_output.append(temp_next)\n",
    "                    \n",
    "                    tape = np.roll(tape, -1)\n",
    "                    tape[-1] = (temp_next)\n",
    "                    \n",
    "                if sim == 0:\n",
    "                    output_acc = np.array(temp_output)\n",
    "                else:\n",
    "                    output_acc += np.array(temp_output)\n",
    "            \n",
    "            output = output_acc / mc_depth \n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Differencer():\n",
    "    # Converts non stationairy (properties change depend on time period of observation) data to stationairy data\n",
    "    # by taking the change in the data (each point becomes the difference between itself and the previous value)\n",
    "    # k (order) times.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def augmented_Dickey_Fuller_test(self, data, sig_lev=0.05):\n",
    "        # the ADF test, useful for finding the order of differences that are necessary to produce stationairy data.\n",
    "        # returns whether the data at this level of difference is stationairy. Statistics so is inaccurate at small sample sizes\n",
    "        result = adfuller(data)\n",
    "        # result[1] is the p value of the test, and if it is less than 0.05, the data is significant and we reject h0\n",
    "        # so no unit root ergo it's stationairy\n",
    "        return (result[1] < sig_lev)\n",
    "\n",
    "    \n",
    "    def stationairy_conv(self, data, significance_level=0.05):\n",
    "        output = np.array(data)\n",
    "        while not self.augmented_Dickey_Fuller_test(output, sig_lev = significance_level):\n",
    "            output = self.difference(output)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def difference(self, data):\n",
    "        base = np.array(data)\n",
    "        return np.concatenate((np.zeros(1), base[1:] - base[:-1]))\n",
    "    \n",
    "    def inv_difference(self, original, differenced_data, order=1):\n",
    "        differenced_data[0] = original\n",
    "        for i in range(order):\n",
    "            differenced_data = np.cumsum(differenced_data)\n",
    "        return differenced_data\n",
    "    \n",
    "class Statistics():\n",
    "    # This is just some statistic tests that I'm working with.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def partial_autocorrelation_test(self, data, maximum_degree=None):\n",
    "        # the PACF test, useful for finding the lag to use for an Autoregressive model (AR)\n",
    "        return pacf(data, nlags=maximum_degree)\n",
    "\n",
    "    def autocorrelation_test(self, data, maximum_degree=None):\n",
    "        # the ACF test, useful for finding the lag to using for a Moving Average model (MA)\n",
    "        if maximum_degree is None:\n",
    "            maximum_degree = math.floor(math.sqrt(len(data)))\n",
    "            # the maximum lag coefficient that will be tested.\n",
    "        dataframe = pd.DataFrame(data)\n",
    "        shifted_dataframe = pd.concat([dataframe.shift(-k) for k in range(0, maximum_degree + 1)], axis = 1)\n",
    "        # The correlation between t and itself is always 1 by the way\n",
    "        correlation = shifted_dataframe.corr()\n",
    "        correlation.columns = [f't-{k}' for k in range(0, maximum_degree + 1)]\n",
    "\n",
    "        data = correlation.iloc[0, 0:]\n",
    "    \n",
    "        return data.to_numpy()\n",
    "\n",
    "\n",
    "        \n",
    "time_signal = np.arange(len(avg_haemo_func_tapping.data[0])) * (1/signal_rate)      \n",
    "displays = display(time_signal)\n",
    "norms = Normalisations()\n",
    "convs = Conversions()\n",
    "evals = Evaluations()\n",
    "diff = Differencer()\n",
    "ar_model = Auto_Regressive_Model(6)\n",
    "generator = Generations()\n",
    "stats = Statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba113d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pACF_result = stats.partial_autocorrelation_test(differenced_data)\n",
    "ACF_result = stats.autocorrelation_test(differenced_data)\n",
    "\n",
    "displays.plot_pChannel(pACF_result, range(len(pACF_result)))\n",
    "displays.plot_pChannel(ACF_result, range(len(ACF_result)))\n",
    "print(pACF_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f98d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displays.plot_pChannel(time_state_data[0], np.arange(0, len(time_state_data[0] - 1)))\n",
    "\n",
    "base_state_value, differenced_data = time_state_data[0][0], diff.difference(time_state_data[0])\n",
    "# displays.plot_pChannel(differenced_data, np.arange(0, len(time_state_data[0] - 1)))\n",
    "\n",
    "# pACF_result = stats.partial_autocorrelation_test(time_state_data[0])\n",
    "# ACF_result = stats.autocorrelation_test(time_state_data[0])\n",
    "\n",
    "# displays.plot_pChannel(pACF_result, range(len(pACF_result)))\n",
    "# displays.plot_pChannel(ACF_result, range(len(ACF_result)))\n",
    "# print(diff.augmented_Dickey_Fuller_test(time_state_data[0]), diff.augmented_Dickey_Fuller_test(differenced_data))\n",
    "ar_rsd = Auto_Regressive_Model(8)\n",
    "# Autoregressive model for resting state data (rsd)\n",
    "\n",
    "base_state_val, differenced_state_data = time_state_data[0][0], diff.difference(time_state_data[0])\n",
    "\n",
    "ar_rsd.fit(time_state_data[0][:500])\n",
    "data_to_use = ar_rsd.predict(time_state_data[0][:10], len(time_state_data[0]), 3, iterative_averaging=True)\n",
    "displays.plot_pChannel(data_to_use, np.arange(len(time_state_data[0])))\n",
    "displays.plot_pChannel(time_state_data[0], np.arange(0, len(time_state_data[0] - 1)))\n",
    "\n",
    "# ar_rsd.fit(differenced_data)\n",
    "# undiffed_data = diff.inv_difference(base_state_val, ar_rsd.predict(differenced_data, len(differenced_data), 200, iterative_averaging=True))\n",
    "# displays.plot_pChannel(diff.inv_difference(base_state_val, differenced_data), range(len(differenced_data)))\n",
    "# displays.plot_pChannel(undiffed_data, range(len(undiffed_data)))\n",
    "print(np.std(data_to_use), np.mean(data_to_use))\n",
    "print(np.std(time_state_data[0]), np.mean(time_state_data))\n",
    "# displays.plot_pChannel(diff.inv_difference(base_state_value, differenced_data), np.arange(0, len(time_state_data[0] - 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(data_to_use), np.mean(data_to_use))\n",
    "print(np.std(time_state_data[0]), np.mean(time_state_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc59a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = random.randint(0, len(data_to_use) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = random.randint(0, len(data_to_use) - 1)\n",
    "displays.plot_pChannel(data_to_use[ra: ra+ 200], range(200))\n",
    "displays.plot_pChannel(time_state_data[0][ra: ra+ 200], range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation models\n",
    "class Auto_Regressive_Model():\n",
    "    # Uses the direct relation between the previous k (order) values to forecast the subsequent value in a time series\n",
    "    # Suitable for when the ACF plot has a slowly decreasing tail, with the highest significant PACF value being order\n",
    "    def __init__(self, order):\n",
    "        self.k = order\n",
    "        self.model = LinearRegression()\n",
    "        self.std = None\n",
    "    \n",
    "    def generate_training_data(self, data):\n",
    "        # returns the inp. data, an array of k long sequences, and outp. data, array of the respective k+i+1'th values\n",
    "        n = len(data)\n",
    "                \n",
    "        x_data = np.reshape(data[:n - self.k], (-1, 1))\n",
    "        for i in range(1, self.k):\n",
    "            x_data = np.hstack((x_data, np.reshape(data[i: n - self.k + i], (-1, 1))))\n",
    "\n",
    "        y_data = data[self.k:]\n",
    "        return np.array(x_data), np.array(y_data)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.std = np.std(data)\n",
    "        training_x, training_y = self.generate_training_data(data)\n",
    "        self.model.fit(training_x, training_y)\n",
    "    \n",
    "    def predict(self, data, steps, mc_depth, iterative_averaging=False):\n",
    "        # Steps refers the number of points that it will return in the future, and mc_depth is the number of monte \n",
    "        # carlo simulations that we'll average to return the data\n",
    "        assert(mc_depth > 0 and steps > 0)\n",
    "        \n",
    "        inputted, output = np.array(data), []\n",
    "        tape = inputted[-self.k:]\n",
    "        if iterative_averaging:\n",
    "            # This approach predicts the next value r (mc_depth) times, then averages that and uses it as the subq. input\n",
    "            # reduces compounding error from consecutive inaccurate predictions, but is more comp. expensive and still \n",
    "            # can have bias if it accumalates over many predictions.\n",
    "            for step in range(steps):\n",
    "                temp_next = 0\n",
    "                \n",
    "                for sim in range(mc_depth):\n",
    "                    temp_next += (self.model.predict(np.reshape(tape, (1, -1))).item() + np.random.normal(scale = self.std))\n",
    "                    # we add an error term to prediction to conser the unpredictability and also for some non-determinism\n",
    "                \n",
    "                tape = np.roll(tape, -1)\n",
    "                next_point = temp_next / mc_depth\n",
    "                tape[-1] = next_point\n",
    "                output.append(next_point)\n",
    "            \n",
    "            output = np.array(output)\n",
    "            \n",
    "        else:\n",
    "            # averaging r (mc_depth) different simulations of k (order) points\n",
    "            \n",
    "            output_acc = np.array([])\n",
    "            # Accumulates all of the simulations\n",
    "            \n",
    "            for sim in range(mc_depth):\n",
    "                tape = inputted[-self.k:]\n",
    "                temp_output = []\n",
    "                \n",
    "                for step in range(steps):\n",
    "                    temp_next = self.model.predict(np.reshape(tape, (1, -1))).item() + np.random.normal(scale = self.std)\n",
    "                    temp_output.append(temp_next)\n",
    "                    \n",
    "                    tape = np.roll(tape, -1)\n",
    "                    tape[-1] = (temp_next)\n",
    "                    \n",
    "                if sim == 0:\n",
    "                    output_acc = np.array(temp_output)\n",
    "                else:\n",
    "                    output_acc += np.array(temp_output)\n",
    "            \n",
    "            output = output_acc / mc_depth \n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Differencer():\n",
    "    # Converts non stationairy (properties change depend on time period of observation) data to stationairy data\n",
    "    # by taking the change in the data (each point becomes the difference between itself and the previous value)\n",
    "    # k (order) times.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def augmented_Dickey_Fuller_test(self, data, sig_lev=0.05):\n",
    "        # the ADF test, useful for finding the order of differences that are necessary to produce stationairy data.\n",
    "        # returns whether the data at this level of difference is stationairy. Statistics so is inaccurate at small sample sizes\n",
    "        result = adfuller(data)\n",
    "        # result[1] is the p value of the test, and if it is less than 0.05, the data is significant and we reject h0\n",
    "        # so no unit root ergo it's stationairy\n",
    "        return (result[1] < sig_lev)\n",
    "\n",
    "    \n",
    "    def stationairy_conv(self, data, significance_level=0.05):\n",
    "        output = np.array(data)\n",
    "        while not self.augmented_Dickey_Fuller_test(output, sig_lev = significance_level):\n",
    "            output = self.difference(output)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def difference(self, data):\n",
    "        base = np.array(data)\n",
    "        return np.concatenate((np.zeros(1), base[1:] - base[:-1]))\n",
    "    \n",
    "    def inv_difference(self, original, differenced_data, order=1):\n",
    "        differenced_data[0] = original\n",
    "        for i in range(order):\n",
    "            differenced_data = np.cumsum(differenced_data)\n",
    "        return differenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWH_ToTensor:\n",
    "    # Channels, then width then height. Basic toTensor() from transforms does not support data of my shape.\n",
    "    def __call__(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "class Log_Transform:\n",
    "    def __call__(self, data, base = norm_base):\n",
    "        return np.emath.logn(base, data) .astype('float32')\n",
    "    # This doesn't need an adjustment for zero because np.log1p just returns zero for that.\n",
    "    # An adjustment would also throw very big errors in because a small adjustment would mean very negative outputs.\n",
    "        \n",
    "\n",
    "pad_log_transform = transforms.Compose([\n",
    "#     Log_Transform(),\n",
    "    CWH_ToTensor()\n",
    "])\n",
    "\n",
    "simple_to_tensor = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.astype('float32')),\n",
    "    transforms.ToTensor()\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bf414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Linear_regressor(torch.nn.Module):\n",
    "#     # https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817 <-- tutorial for this\n",
    "#     def __init__(self):\n",
    "#         super().__init__()  \n",
    "#         self.linear_layer = None\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         pred = self.linear_layer(x)\n",
    "#         return pred\n",
    "    \n",
    "#     def fit(self, x_train, y_train, epoch_num=50):\n",
    "#         print(np.shape(x_train), np.shape(y_train))\n",
    "#         # Only works with one dimensional data because I don't need it to do anything beyond that.\n",
    "#         self.linear_layer = torch.nn.Linear(len(x_train[0]), len(y_train[0]))\n",
    "#         loss_func = torch.nn.MSELoss() \n",
    "#         optimiser = torch.optim.SGD(self.parameters(), lr=1e-4)\n",
    "        \n",
    "#         for epoch in range(epoch_num):\n",
    "#             inputs = Variable(torch.from_numpy(x_train))\n",
    "#             labels = Variable(torch.from_numpy(y_train))\n",
    "            \n",
    "#             optimiser.zero_grad()\n",
    "#             outputs = self(inputs)\n",
    "            \n",
    "#             pred_loss = loss_func(outputs, labels)\n",
    "#             pred_loss.backward()\n",
    "#             optimiser.step()\n",
    "            \n",
    "#         print(f\"Final loss of {pred_loss.item()%.4}\")\n",
    "        \n",
    "#     def predict(self, tape):\n",
    "#         return self(torch.from_numpy(tape))\n",
    "    \n",
    "# class AutoRegression:\n",
    "#     def __init__(self, order):\n",
    "#         self.order = order \n",
    "#         # The order of an AutoRegression model is the number of previous variables used to calculate the next one\n",
    "#         self.model = Linear_regressor()\n",
    "#         self.std = None\n",
    "        \n",
    "#     def generate_predictor_data(self, data_set):\n",
    "#         n = len(data_set)\n",
    "        \n",
    "#         data = data_set[:n - self.order]\n",
    "#         data = np.reshape(data, (-1, 1))\n",
    "        \n",
    "#         # This takes the values from index 0 up to n-order and reshapes them into a column vector \n",
    "#         # We then stack the values from index 1 to (n-order) + 1. \n",
    "        \n",
    "#         # If we hadn't done 0 independently, we'd have nothing to stack onto\n",
    "#         for i in range(1, self.order):\n",
    "#             temp_col = data_set[i:(n+i) - self.order]\n",
    "#             temp_col = np.reshape(temp_col, (-1, 1))\n",
    "#             np.hstack((data, temp_col))\n",
    "            \n",
    "#         return data\n",
    "    \n",
    "#     def generate_responce_data(self, data_set):\n",
    "#         return data_set[self.order:]\n",
    "    \n",
    "#     def fit(self, data_set):\n",
    "#         self.std = np.std(data_set)\n",
    "#         t_x_data = self.generate_predictor_data(data_set)  # X data for training\n",
    "#         t_y_data = self.generate_responce_data(data_set)  # Y data for training\n",
    "#         self.model.fit(t_x_data, t_y_data)\n",
    "        \n",
    "#     def predict(self, data_set, step_count, simulation_num):\n",
    "#         data_set = np.array(data_set)\n",
    "#         output = np.array([])\n",
    "        \n",
    "#         for sim in range(simulation_num):\n",
    "#             # We do a monte carlo simulation approach, performing a bunch of simulations, then averaging them\n",
    "#             temp_ans = []\n",
    "#             tape = data_set[-self.order:]\n",
    "#             # Take the last order num values as a rolling tape measure, then keep rolling back to the start\n",
    "#             # along the tape, putting in our predicted values in as we go\n",
    "            \n",
    "#             for prediction in range(step_count):\n",
    "#                 predicted = self.model.predict(np.reshape(tape, (-1, 1)))\n",
    "#                 predicted += np.random.normal(loc=0, scale=self.std)\n",
    "#                 # We predict the next value, then add some gaussian noise to it depending on the standard\n",
    "#                 # deviation of the original dataset we were using...\n",
    "                \n",
    "#                 temp_ans.append(predicted[0].detach().numpy())\n",
    "#                 tape = np.roll(tape, -1) \n",
    "#                 # We move the tape backwards by one, the last value becomes the second to last and so forth\n",
    "#                 tape[-1] = predicted[0].detach().numpy()\n",
    "                \n",
    "#             if sim == 0:\n",
    "#                 output = np.array(temp_ans)\n",
    "#             else:\n",
    "#                 output = output + np.array(temp_ans)\n",
    "#                 # since we'll be averaging out the values anyway, just add them elementwise for now\n",
    "#         output = output/ simulation_num\n",
    "#         return output    \n",
    "    \n",
    "    \n",
    "# auto_regression = AutoRegression(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class padded_wavelet_database:\n",
    "    def __init__(self, clean, overlays, transform=pad_log_transform, base = norm_base, add_len = 0, inherit_params = None):\n",
    "        # Both clean and overlays are in wavelet form by the way\n",
    "        self.lengths = np.array([len(row) for row in clean[0]])\n",
    "        self.add_len = add_len\n",
    "        self.clean = self.pad(clean, max(self.lengths), add_len = add_len)\n",
    "        self.dirty = self.pad(overlays, max(self.lengths), add_len = add_len) #+ self.clean\n",
    "        self.transform = pad_log_transform\n",
    "        \n",
    "        if inherit_params is not None:\n",
    "            self.min = inherit_params.min\n",
    "            self.max = inherit_params.max\n",
    "            self.mean = inherit_params.mean\n",
    "            self.std = inherit_params.std\n",
    "        \n",
    "        else:\n",
    "            self.min = np.min(self.dirty[self.dirty != 0])\n",
    "            self.max = np.max(self.dirty[self.dirty != 0])\n",
    "            self.mean = np.mean(self.clean[self.clean != 0])\n",
    "            self.std = np.std(self.clean[self.clean != 0])\n",
    "\n",
    "\n",
    "        self.clean[self.clean != 0] = np.emath.logn(norm_base, 1 + ((self.clean[self.clean != 0] - self.min) / (self.max - self.min)))\n",
    "        self.dirty[self.dirty != 0] = np.emath.logn(norm_base, 1 + ((self.dirty[self.dirty != 0] - self.min) / (self.max - self.min)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clean)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        # Basically, the expected shape has one additional dimension, that of channels, that is not in my data, \n",
    "        # so i need to expand it by one so it has [channels (1), height, width]\n",
    "        return (self.transform(np.expand_dims(self.clean[i].astype('float32'), axis=0)),\n",
    "            self.transform(np.expand_dims(self.dirty[i].astype('float32'), axis=0)))\n",
    "        \n",
    "    \n",
    "    def pad(self, wavedeck, max_length, add_len = 0):\n",
    "        # wavedeck is an array of the products of a wavedec of the signal. cause its a deck of wavedec's. lol.\n",
    "        padded_wavedecs = np.array([\n",
    "        [np.concatenate((np.array(row), np.zeros(max_length + add_len - len(row)))) \n",
    "             for row in signal] \n",
    "                for signal in wavedeck])\n",
    "        return padded_wavedecs\n",
    "    \n",
    "    def get_signal_back(self, signal, original_lengths, base = norm_base):\n",
    "        recreation = []\n",
    "        \n",
    "        unnormalised_ver = np.zeros(np.shape(signal[0]))\n",
    "        unnormalised_ver += ((np.array(base ** (signal[0])) - 1) * (self.max - self.min)) + self.min\n",
    "        # signal[0], because we added an extra dimension previously\n",
    "        \n",
    "        for i, layer in enumerate(unnormalised_ver):\n",
    "            recreation.append(layer[:original_lengths[i]])\n",
    "            \n",
    "        return convs.waverec(recreation) # Can sometimes give back signals that are too long, just cut to size.\n",
    "    \n",
    "    def mass_signal_revert(self, signals, original_lengths):\n",
    "        reverted_signals = []\n",
    "        \n",
    "        for signal in signals:\n",
    "            reverted_signals = self.get_signal_back(signal, original_lengths)\n",
    "            \n",
    "        return np.array(reverted_signals)\n",
    "    \n",
    "    \n",
    "class simple_amplitude_database:\n",
    "    def __init__(self, clean, overlays, transform=simple_to_tensor, base = norm_base):\n",
    "        # Both clean and overlays are in the format of a time series of amplitudes\n",
    "        self.clean = clean\n",
    "        self.dirty = clean + overlays\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.min = np.min(self.dirty)\n",
    "        self.max = np.max(self.dirty)\n",
    "        self.mean = np.mean(self.dirty)\n",
    "        self.std = np.std(self.dirty)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dirty)\n",
    "    \n",
    "    def z_normalise(self, arr):\n",
    "        return (arr - self.mean) / self.std\n",
    "    \n",
    "    def inv_z_normalise(self, arr):\n",
    "        return (arr * self.std) + self.mean\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        clean_z_expanded = np.expand_dims(self.z_normalise(self.clean[i]), axis=0)\n",
    "        dirty_z_expanded = np.expand_dims(self.z_normalise(self.dirty[i]), axis=0)\n",
    "        \n",
    "        clean_transformed = self.transform(clean_z_expanded)\n",
    "        dirty_transformed = self.transform(dirty_z_expanded)\n",
    "\n",
    "        return (clean_transformed[0],\n",
    "               dirty_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973b236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target, overlay = generator.generate_basic_test_data(1000)\n",
    "eval_target, eval_overlay = generator.generate_basic_test_data(250)\n",
    "test_target, test_overlay = generator.generate_basic_test_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the setup code for the spectra representation versions\n",
    "\n",
    "target_wavelets = [convs.wavedec(wave_signal) for wave_signal in target]\n",
    "overlay_wavelets = [convs.wavedec(wave_signal) for wave_signal in (target + overlay)]\n",
    "padded_set = padded_wavelet_database(target_wavelets, overlay_wavelets, add_len = 5)\n",
    "padded_dataloader = DataLoader(padded_set, batch_size=64, shuffle=True)\n",
    "\n",
    "eval_wavelets = [convs.wavedec(wave_signal) for wave_signal in eval_target]\n",
    "eval_overlay_wavelets = [convs.wavedec(wave_signal) for wave_signal in (eval_target + eval_overlay)]\n",
    "eval_padded_set = padded_wavelet_database(eval_wavelets, eval_overlay_wavelets, add_len = 5)\n",
    "eval_data = DataLoader(eval_padded_set, batch_size=64, shuffle=True)\n",
    "\n",
    "test_wavelets = [convs.wavedec(wave_signal) for wave_signal in test_target]\n",
    "\n",
    "simple_set = simple_amplitude_database(target[:, :224], overlay[:, :224])\n",
    "simple_dataloader = DataLoader(simple_set, batch_size=64, shuffle=True)\n",
    "\n",
    "simple_test_set = simple_amplitude_database(test_target[:, :224], test_overlay[:, :224])\n",
    "simple_test_dataloader = DataLoader(simple_test_set, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new set of test data\n",
    "test_target, test_overlay = generator.generate_basic_test_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class regular_CNN_AE(nn.Module):\n",
    "    # A convolutional neural network that works on regular 2d image representations of the signal,\n",
    "    # whether padded or stretched\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 12, 1, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(12, 48, 3, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Conv2d(48, 96, 3, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(96, 48, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ConvTranspose2d(48, 12, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ConvTranspose2d(12, 1, 3, stride=1, padding=1, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.Sigmoid() # This converts the output to be between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "    \n",
    "class heavy_CNN_AE(nn.Module):\n",
    "    # A convolutional neural network that works on regular 2d image representations of the signal,\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 12, 1, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.Conv2d(12, 36, 1, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(36, 144, 3, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            nn.Conv2d(144, 288, 3, stride=1, padding=0, bias=True, padding_mode='zeros'),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(288, 144, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            \n",
    "            nn.ConvTranspose2d(144, 72, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(72, 24, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=0.15),\n",
    "            \n",
    "            nn.ConvTranspose2d(24, 12, 3, stride=1, padding=0, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.ConvTranspose2d(12, 1, 1, stride=1, padding=2, output_padding=0, bias=True, padding_mode='zeros'),\n",
    "            nn.Sigmoid() # This converts the output to be between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "\n",
    "class v3_CNN_AE(nn.Module):\n",
    "    # Convolutional neural network following a similar style to that of Yuanyuan gao. et al's approach, with\n",
    "    # max pooling on a static number of channels in the encoder, then upsampling with a static number of channels \n",
    "    # in the decoder. \n",
    "    \n",
    "    # PS The input sample window MUST be of an EVEN LENGTH\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2, padding = 0),\n",
    "            nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.Conv1d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2, padding = 0),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "            \n",
    "            nn.Conv1d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2, padding = 0),\n",
    "            \n",
    "            nn.Conv1d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2, padding = 0),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "\n",
    "            nn.Conv1d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "            \n",
    "            nn.Conv1d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            \n",
    "            nn.Conv1d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            \n",
    "            nn.Conv1d(32, 1, 3, padding=1, stride=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "    \n",
    "class v4_CNN_AE(nn.Module):\n",
    "    # This is applying Yuanyuan Gao et al.'s approach to training a denoising autoencoder for linear time serise\n",
    "    # and applying it to a spectogram representation.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool2d(2, padding = 1),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool2d(2, padding = 0),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 32, 3, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, padding = 0),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "            nn.Conv2d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "            nn.Conv2d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(32, 32, 3, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.Conv2d(32, 32, 4, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "            nn.Dropout(p=0.1),     \n",
    "            \n",
    "            nn.Conv2d(32, 1, 4, padding = 1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        reconstruction = self.decoder(code)\n",
    "        return reconstruction\n",
    "        \n",
    "basic_padded_model = regular_CNN_AE()\n",
    "heavy_padded_model = heavy_CNN_AE()\n",
    "simple_time_model = v3_CNN_AE()\n",
    "maxPooled_specto_model = v4_CNN_AE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_train_model(model_type, dataloader, loss_function=nn.MSELoss, epoch_number = 50, learning_rate = 1e-3):\n",
    "    denoising_model = model_type()\n",
    "    loss_func = loss_function()\n",
    "    optimiser = optim.Adam(denoising_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        for (clean, dirty) in dataloader:\n",
    "#             print(np.shape(clean), np.shape(dirty))\n",
    "            reconstructed = denoising_model(dirty)\n",
    "#             print(np.shape(reconstructed), np.shape(clean), np.shape(dirty))\n",
    "            reconstruction_loss = loss_func(reconstructed, clean)\n",
    "            optimiser.zero_grad()\n",
    "            reconstruction_loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        print(f\"The loss in epoch {epoch + 1} of {epoch_number} epochs is {reconstruction_loss.item()%.4}\")\n",
    "    \n",
    "    return denoising_model\n",
    "\n",
    "def basic_eval_model(model, dataset, loss_function, display_num = 1):\n",
    "    loss_acc = 0.\n",
    "    dataloader = DataLoader(dataset, batch_size = 20, shuffle = False)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (clean, dirty) in dataloader:\n",
    "            reconstructed = model(dirty)\n",
    "            loss_acc += loss_function(reconstructed.detach().numpy(), clean.detach().numpy())\n",
    "    \n",
    "    print(f\"Total loss of {loss_acc}, with a mean loss of {loss_acc / len(dataloader)} in a sample size of {len(dataset)}\")\n",
    "    \n",
    "    display_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    iter_num = 0\n",
    "    for display_clean, display_dirty in display_dataloader:\n",
    "        if iter_num < display_num:\n",
    "            display_recon = model(display_dirty).detach().numpy()[0][0]\n",
    "            displays.plot_comparision_data(display_clean[0][0], display_recon, np.arange(len(display_recon)) * sample_rate) \n",
    "            displays.plot_comparision_data(display_dirty[0][0], display_recon, np.arange(len(display_recon)) * sample_rate) \n",
    "            iter_num += 1\n",
    "            \n",
    "def wavelet_eval_model(model, dataset, loss_function, display_num = 1):\n",
    "    # takes in a padded dataset by the way, \n",
    "    loss_acc = 0.\n",
    "    dataloader = DataLoader(dataset, batch_size = 20, shuffle = False)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (clean, dirty) in dataloader:\n",
    "            reconstructed = model(dirty)\n",
    "            loss_acc += loss_function(reconstructed.detach().numpy(), clean.detach().numpy())\n",
    "    \n",
    "    print(f\"Total loss of {loss_acc}, with a mean loss of {loss_acc / len(dataloader)} in a sample size of {len(dataset)}\")\n",
    "    \n",
    "    display_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    iter_num = 0\n",
    "    for display_clean, display_dirty in display_dataloader:\n",
    "        if iter_num < display_num:          \n",
    "            display_recon = model(display_dirty).detach().numpy()[0]\n",
    "            displays.plot_comp_wavedec_layer(display_clean[0][0].detach().numpy(), display_recon[0])\n",
    "#             displays.plot_comp_wavedec_layer(display_dirty[0][0].detach().numpy(), display_recon[0])\n",
    "            data_rev_rec = dataset.get_signal_back(display_recon, dataset.lengths)\n",
    "            data_rev_cle = dataset.get_signal_back(display_clean[0].detach().numpy(), dataset.lengths)\n",
    "            displays.plot_comparision_data(data_rev_rec, data_rev_cle, range(len(data_rev_cle)))\n",
    "            iter_num += 1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d7ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxPooled_specto_model = basic_train_model(v4_CNN_AE, padded_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ec2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simp_model = basic_train_model(v3_CNN_AE, simple_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f7aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_target, test_overlay = generator.generate_basic_test_data(100)\n",
    "simple_test_set = simple_amplitude_database(test_target[:, :224], test_target[:, :224] + test_overlay[:, :224])\n",
    "\n",
    "test_target_wavelets, test_overlay_wavelets = [convs.wavedec(wave_signal) for wave_signal in test_target], [convs.wavedec(wave_signal) for wave_signal in (test_target + test_overlay)]\n",
    "test_padded_dataset = padded_wavelet_database(test_target_wavelets, test_overlay_wavelets, add_len = 5, inherit_params = padded_dataset)\n",
    "\n",
    "basic_eval_model(simp_model, simple_test_set, evals.root_mean_square_error, display_num=3)\n",
    "wavelet_eval_model(maxPooled_specto_model, test_padded_dataset, evals.root_mean_square_error, display_num=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f168a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_target_wavelets, test_overlay_wavelets = [convs.wavedec(wave_signal) for wave_signal in test_target], [convs.wavedec(wave_signal) for wave_signal in (test_target+ test_overlay)]\n",
    "test_padded_dataset = padded_wavelet_database(test_target_wavelets, test_overlay_wavelets, add_len = 5)\n",
    "\n",
    "wavelet_eval_model(maxPooled_specto_model, test_padded_dataset, evals.root_mean_square_error, display_num=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20e4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_eval_model(simp_model, simple_test_set, evals.root_mean_square_error, display_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90ede2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len_Adder = 5\n",
    "loader = DataLoader(test_padded_dataset, batch_size=1, shuffle=False)\n",
    "maxs = 1\n",
    "for (clean, dirty) in loader:\n",
    "    if maxs > 0:\n",
    "        displays.plot_pChannel(test_padded_dataset.get_signal_back(dirty.detach().numpy()[0], test_padded_dataset.lengths), range(len(test_padded_dataset.get_signal_back(dirty.detach().numpy()[0], test_padded_dataset.lengths))))\n",
    "        reconser = maxPooled_specto_model(dirty)\n",
    "        displays.plot_pChannel(test_padded_dataset.get_signal_back(reconser.detach().numpy()[0], test_padded_dataset.lengths), range(len(test_padded_dataset.get_signal_back(reconser.detach().numpy()[0], test_padded_dataset.lengths))))\n",
    "\n",
    "        maxs -= 1\n",
    "wavetester = padded_set.pad(test_wavelets, max(padded_set.lengths), add_len=len_Adder)[0]\n",
    "\n",
    "\n",
    "#torch.tensor(padded_set.pad(test_wavelet[0], add_len=5), dtype=torch.float32)\n",
    "\n",
    "# maxPooled_specto_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3197e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

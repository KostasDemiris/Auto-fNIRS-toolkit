{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5033a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "import random\n",
    "\n",
    "from scipy.signal import stft, istft\n",
    "from scipy.signal.windows import hann, gaussian\n",
    "from scipy.special import gamma\n",
    "from scipy.optimize import curve_fit\n",
    "import pywt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9f4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables to be put here to be varied?\n",
    "time_win = 5\n",
    "sc_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3171723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/kostasdemiris/mne_data/MNE-fNIRS-motor-data/Participant-1\n",
      "Reading 0 ... 23238  =      0.000 ...  2974.464 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.05 - 0.7 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.05\n",
      "- Lower transition bandwidth: 0.02 Hz (-6 dB cutoff frequency: 0.04 Hz)\n",
      "- Upper passband edge: 0.70 Hz\n",
      "- Upper transition bandwidth: 0.20 Hz (-6 dB cutoff frequency: 0.80 Hz)\n",
      "- Filter length: 1291 samples (165.248 s)\n",
      "\n",
      "Used Annotations descriptions: ['Control', 'Tapping/Left', 'Tapping/Right']\n",
      "Not setting metadata\n",
      "90 matching events found\n",
      "Setting baseline interval to [-1.024, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 90 events and 204 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset that we're going to use\n",
    "fnirs_folder = mne.datasets.fnirs_motor.data_path()\n",
    "fnirs_cw_amplitude_dir = fnirs_folder / \"Participant-1\"\n",
    "raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir, verbose=True)\n",
    "raw_intensity.load_data()\n",
    "\n",
    "\n",
    "\n",
    "raw_intensity.annotations.set_durations(time_win)\n",
    "raw_intensity.annotations.rename(\n",
    "    {\"1.0\": \"Control\", \"2.0\": \"Tapping/Left\", \"3.0\": \"Tapping/Right\"}\n",
    ")\n",
    "unwanted = np.nonzero(raw_intensity.annotations.description == \"15.0\")\n",
    "raw_intensity.annotations.delete(unwanted)\n",
    "# Apparently channel 15 is unrelated to the motor activation experiment,\n",
    "# it was used to signal something unrelated and so will be ignored\n",
    "\n",
    "\n",
    "\n",
    "# Removes short channels\n",
    "picks = mne.pick_types(raw_intensity.info, meg = False, fnirs = True)\n",
    "dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "    raw_intensity.info, picks = picks)\n",
    "raw_intensity.pick(picks[dists > 0.01])\n",
    "\n",
    "\n",
    "\n",
    "# Converting to optical density based on readings, then to haemo readings\n",
    "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=6)\n",
    "\n",
    "# We filter them to remove the frequencies associated with cardiac activity (unrelated)\n",
    "raw_haemo_unfiltered = raw_haemo.copy()\n",
    "raw_haemo.filter(0.05, 0.7, h_trans_bandwidth = 0.2, l_trans_bandwidth = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "# Creates epochs for each occurence of an event (Left tap, right tap, control)\n",
    "reject_criteria = dict(hbo=80e-6)\n",
    "tmin, tmax = -1, 25\n",
    "events, event_dict = mne.events_from_annotations(raw_haemo)\n",
    "\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw_haemo,\n",
    "    events,\n",
    "    event_id=event_dict,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    reject=reject_criteria,\n",
    "    reject_by_annotation=True,\n",
    "    proj=True,\n",
    "    baseline=(None, 0),\n",
    "    preload=True,\n",
    "    detrend=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "isolated_event_epochs = epochs[\"Tapping\"]\n",
    "control_event_set = epochs[\"Control\"]\n",
    "isol_dhrf_data = isolated_event_epochs.get_data(copy=True)\n",
    "avg_haemo_func_tapping = isolated_event_epochs.average()\n",
    "std_dev = np.std(avg_haemo_func_tapping.data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion between the different forms i need, including for display purposes.\n",
    "class convs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply_stft(self, signal, sample_rate):\n",
    "        window = gaussian(128, std = 7)\n",
    "        hop = 128\n",
    "        # returns in the format: frequency, time, result\n",
    "        return stft(signal, sample_rate, nperseg = 64)\n",
    "    \n",
    "    def wavelet_decmp(self, signal, m_wavelet, d_level):\n",
    "         return pywt.wavedec(signal, m_wavelet, d_level)\n",
    "    \n",
    "    def i_wavelet_decmp(self, coeffs, m_wavelet):\n",
    "        return pywt.waverec(coeffs, m_wavelet)\n",
    "    \n",
    "    def inv_WvImg_conv(self, wvImg, arr_shape, minim_data, maxim_data):\n",
    "        # Firstly seperate it into channels, for each row of pixels\n",
    "        # remove all padded values, un-normalise then inverse_wavelet_conv\n",
    "        un_padded = []\n",
    "        un_norm = rev_normalise(wvImg, minim_data, maxim_data)\n",
    "        for row in un_norm:\n",
    "            un_padded.append(self.i_wavelet_decmp(rem_padding(row, arr_shape), wavelet))\n",
    "        return un_padded\n",
    "    \n",
    "    def padarray(self, arr, size): \n",
    "        return np.pad(arr, pad_width=(0, size-len(arr)), mode='constant')\n",
    "    \n",
    "    def rem_padding(self, arr, arr_shape):\n",
    "        original_array = []\n",
    "        sec = max(arr_shape)\n",
    "        start_i = 0\n",
    "        while start_i * sec < len(arr) - 1:\n",
    "            original_array.append(arr[start_i * sec : (start_i * sec) + arr_shape[start_i]])\n",
    "            start_i += 1\n",
    "        return original_array\n",
    "    \n",
    "    def normalise(self, arr, scale_factor=sc_factor):\n",
    "        data_min = np.min(arr)\n",
    "        data_max = np.max(arr)\n",
    "        return ((arr - data_min) / (data_max - data_min)) * scale_factor\n",
    "\n",
    "    def rev_normalise(self, arr, n_min, n_max, scale_factor=sc_factor):\n",
    "        return (((arr-1)/scale_factor) * (n_max - n_min)) + n_min\n",
    "    \n",
    "    # Not necessary if using the base normalise function, just for testing\n",
    "    def event_to_wvImg(self, sample, wavelet, level, min_n, max_n):\n",
    "        wv_channel_arr = []\n",
    "        for channel in sample:\n",
    "            wv_form = convs.wavelet_decmp(channel, wavelet, level)\n",
    "            maxim_len = max(len(sub_arr) for sub_arr in wv_form)\n",
    "            wv_channel_arr.append(np.array([padarray(arr, maxim_len) for arr in wv_form]).flatten())\n",
    "        wv_channel_arr = np.array(wv_channel_arr)\n",
    "        normalised = sc_factor * (wv_channel_arr - min_n) / ( max_n - min_n)\n",
    "        normalised[wv_channel_arr == 0] = -1\n",
    "        return normalised\n",
    "    \n",
    "    def convert_signal(self, signal):\n",
    "        decomposed = convs.wavelet_decmp(signal, wavelet, level)\n",
    "        pad_decomp = np.array([padarray(a[i], len(a[len(a)-1])) for i in range(len(a))]).flatten()\n",
    "        return pad_decomp\n",
    "\n",
    "    def reverse_conversion(self, signal, sig_min, sig_max, sub_arr_lens):\n",
    "        unNorm = rev_normalise(signal, sig_min, sig_max)\n",
    "        unpadded_Ver = rem_padding(unNorm, sub_arr_lens)\n",
    "        return convs.i_wavelet_decmp(unpadded_Ver, wavelet)\n",
    "    \n",
    "    def z_curve_normalisation(self, data):\n",
    "        return (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "\n",
    "convs = convs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7152438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class display:\n",
    "    def __init__(self, time_sig):\n",
    "        self.time_signal = time_sig\n",
    "    \n",
    "    def display_gray_arr(self, img_arr):\n",
    "        plt.imshow(img_arr, cmap='gray')\n",
    "        plt.title(\"Wavelet as Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_pChannel(self, points, time_series):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(time_series, points)\n",
    "        plt.title(\"channel\")\n",
    "        plt.xlabel(\"Time [s]\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_scatter_data(self, data_points, central=0.5):\n",
    "        plt.scatter([i for i in range(len(data_points))], data_points, color='white', alpha=0.5)\n",
    "\n",
    "        for i in range(len(data_points)):\n",
    "            plt.plot([i, i], [data_points[i], central], color='red', alpha=0.5)\n",
    "\n",
    "        plt.axhline(y=central, color='green', linestyle='--')\n",
    "\n",
    "        plt.title('Scatter Plot of Data Points by Index')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "time_signal = np.arange(len(avg_haemo_func_tapping.data[0])) * (1/signal_rate)      \n",
    "displays = display(time_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832b8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual conversion of the data to wavelet form and so forth, for the 60 original signals\n",
    "\n",
    "wavelet = 'sym4' # Resembles cannonical haemo-dynamic responce \n",
    "level = 4\n",
    "first_transform = convs.wavelet_decmp(isol_dhrf_data[0][0], wavelet, level)\n",
    "lengths = [len(sub_array) for sub_array in first_transform]\n",
    "max_length = max(lengths)\n",
    "signal_rate = 7.81\n",
    "indv_coeff_len = len(first_transform)\n",
    "\n",
    "\n",
    "n_data = np.empty((60, 40, (level+1) * max_length))\n",
    "\n",
    "for i in range(0, len(isol_dhrf_data)):\n",
    "    for j in range(0, len(isol_dhrf_data[0])):\n",
    "        temp = convs.wavelet_decmp(isol_dhrf_data[i][j], wavelet, level)\n",
    "        n_data[i][j] = np.array([convs.padarray(arr, max_length) for arr in temp]).flatten()\n",
    "        \n",
    "n_min = np.min(n_data)\n",
    "n_max = np.max(n_data)\n",
    "norm_data = convs.normalise(n_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c38230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnt this off a tutorial that I need to put credit on here at some point\n",
    "class AutoRegression:\n",
    "    def __init__(self, order):\n",
    "        self.order = order \n",
    "        # The order of an AutoRegression model is the number of previous variables used to calculate the next one\n",
    "        self.model = LinearRegression()\n",
    "        self.std = None\n",
    "        \n",
    "    def generate_predictor_data(self, data_set):\n",
    "        n = len(data_set)\n",
    "        \n",
    "        data = data_set[:n - self.order]\n",
    "        data = np.reshape(data, (-1, 1))\n",
    "        \n",
    "        # This takes the values from index 0 up to n-order and reshapes them into a column vector \n",
    "        # We then stack the values from index 1 to (n-order) + 1. \n",
    "        \n",
    "        # If we hadn't done 0 independently, we'd have nothing to stack onto\n",
    "        for i in range(1, self.order):\n",
    "            temp_col = data_set[i:(n+i) - self.order]\n",
    "            temp_col = np.reshape(temp_col, (-1, 1))\n",
    "            np.hstack((data, temp_col))\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def generate_responce_data(self, data_set):\n",
    "        return data_set[self.order:]\n",
    "    \n",
    "    def fit(self, data_set):\n",
    "        self.std = np.std(data_set)\n",
    "        t_x_data = self.generate_predictor_data(data_set)  # X data for training\n",
    "        t_y_data = self.generate_responce_data(data_set)  # Y data for training\n",
    "        self.model.fit(t_x_data, t_y_data)\n",
    "        \n",
    "    def predict(self, data_set, step_count, simulation_num):\n",
    "        data_set = np.array(data_set)\n",
    "        output = np.array([])\n",
    "        \n",
    "        for sim in range(simulation_num):\n",
    "            # We do a monte carlo simulation approach, performing a bunch of simulations, then averaging them\n",
    "            temp_ans = []\n",
    "            tape = data_set[-self.order:]\n",
    "            # Take the last order num values as a rolling tape measure, then keep rolling back to the start\n",
    "            # along the tape, putting in our predicted values in as we go\n",
    "            \n",
    "            for prediction in range(step_count):\n",
    "                predicted = self.model.predict(np.reshape(tape, (-1, 1)))\n",
    "                predicted += np.random.normal(loc=0, scale=self.std)\n",
    "                # We predict the next value, then add some gaussian noise to it depending on the standard\n",
    "                # deviation of the original dataset we were using...\n",
    "                \n",
    "                temp_ans.append(predicted[0])\n",
    "                tape = np.roll(tape, -1) \n",
    "                # We move the tape backwards by one, the last value becomes the second to last and so forth\n",
    "                tape[-1] = predicted[0]\n",
    "                \n",
    "            if sim == 0:\n",
    "                output = np.array(temp_ans)\n",
    "            else:\n",
    "                output = output + np.array(temp_ans)\n",
    "                # since we'll be averaging out the values anyway, just add them elementwise for now\n",
    "        output = output/ simulation_num\n",
    "        return output    \n",
    "    \n",
    "    \n",
    "auto_regression = AutoRegression(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3fb37f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_noise_gen:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_freq(self, frequency, sample_rate, sig_length, amplitude, start_offset=0):\n",
    "        points = []\n",
    "        interval = 1 / sample_rate\n",
    "        curr = 0\n",
    "        while curr + interval <= sig_length:\n",
    "            points.append(amplitude * math.sin(frequency * 2 * math.pi * (curr + start_offset)))\n",
    "            curr += interval\n",
    "        return points\n",
    "    \n",
    "    def generate_noise(self, frequencies, sample_rate, sig_length, max_amp):\n",
    "        noise = np.zeros(math.floor(sample_rate * sig_length))\n",
    "        for freq in frequencies:\n",
    "            noise = noise + self.generate_freq(freq, sample_rate, sig_length, random.uniform(0, max_amp), start_offset=random.uniform(0, math.pi))\n",
    "        return noise\n",
    "    \n",
    "    def generate_white_noise(self, p_num, amplitude=1):\n",
    "        output = np.zeros((p_num))\n",
    "        for i in range(p_num):\n",
    "            output[i] = np.random.random_sample()\n",
    "        return output\n",
    "    \n",
    "    def generate_rolling_noise(self, mean_freq, std_freq, mean_amp, std_amp, sig_length, sample_rate, starting_offset=1):\n",
    "        # Generates noise that changes over time in a \"rolling\" manner - we impose gradually changing trends on it\n",
    "        output = np.zeros((sig_length+20,))\n",
    "        sum_weighted_time = starting_offset\n",
    "        for i in range(len(output)):\n",
    "            output[i] = (math.sin(2 * math.pi * sum_weighted_time) + 1) * np.random.normal(mean_amp, std_amp)\n",
    "            sum_weighted_time += np.random.normal(mean_freq, std_freq) * (1 / sample_rate)\n",
    "        for i in range(10, len(output)-10):\n",
    "            output[i] = sum(output[i-10:i+10]) / 20\n",
    "        return output[10: len(output)-10]\n",
    "    \n",
    "    # generates noise, then temporally correlates it with a base 33 order (overridable)\n",
    "    def generate_tCorr_noise(self, p_num, order=33):\n",
    "        noise_data = self.generate_white_noise(p_num * 2)\n",
    "        auto_regression.fit(noise_data)\n",
    "        time_correlated_data = auto_regression.predict(noise_data, p_num, order)\n",
    "        return time_correlated_data\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "random_gen = random_noise_gen()\n",
    "\n",
    "# displays.plot_scatter_data(noise_generator.generate_rolling_noise(0.1, 1, 0.5, 0.01, 1000, 10))\n",
    "# displays.plot_scatter_data(noise_generator.generate_tCorr_noise(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3b65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bio_modelling:\n",
    "    # This is for the modelling of the biological signals that we're dealing with, as opposed to the mostly mathematicaly stuff from the\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    @staticmethod\n",
    "    def gamma_dist(t, k, theta):\n",
    "        # For a specific time, it returns the value of a gamma distribution with a given k and theta at that time\n",
    "        return  (t ** (k-1)) * (math.e ** (-t / theta)) / (gamma(k) * (theta** k))\n",
    "\n",
    "    @staticmethod\n",
    "    def bC_hrf(t, k1, k2, theta1, theta2):\n",
    "        # returns the basic version of the canonical hrf, modelled as the difference between two gamma functions\n",
    "        return bio_modelling.gamma_dist(t, k1, theta1) - bio_modelling.gamma_dist(t, k2, theta2)\n",
    "    \n",
    "    def generate_phony_signal(self, p_num, generator=random_gen):\n",
    "        if generator is None:\n",
    "            generator = self.generator\n",
    "        time_correlated_points = (generator.generate_tCorr_noise(p_num) - 0.5) * math.pow(10, -9)\n",
    "        heart_rate = generator.generate_rolling_noise(1, 0.25, 0, math.pow(10, -7), p_num, 7.81)\n",
    "        respi_rate = generator.generate_rolling_noise(0.3, 0.1, 0, math.pow(10, -7), p_num, 7.81)\n",
    "        mayer_wavs = generator.generate_rolling_noise(0.1, 0.005, 0, math.pow(10, -7), p_num, 7.81)\n",
    "        return time_correlated_points + heart_rate + respi_rate + mayer_wavs\n",
    "    \n",
    "    def get_basic_gamma_params(self, target_signal, time_axis, initial_paramaters=None):\n",
    "        parameters, covariance = curve_fit(bio_modelling.bC_hrf, time_axis, target_signal, p0=initial_parameters)\n",
    "        # Gradient descent to fit parameters of the function to have it fit the curve\n",
    "        k1, k2, theta1, theta2 = parameters\n",
    "        return k1, k2, theta1, theta2\n",
    "    \n",
    "    def generate_noise_set(self, set_size, signal_length, noise_function=None):\n",
    "        if noise_function is None:\n",
    "            return np.array([self.generate_phony_signal(signal_length+30)[15: signal_length + 15] for point in range(set_size)])\n",
    "        \n",
    "        return np.array([noise_function(signal_length) for point in range(set_size)])\n",
    "    \n",
    "    def generate_gamma_set(self, time_axis, parameters, size, mean=0, var=0.01, canonical=True, magnitude_correction=1):\n",
    "        # generates a set of gamma functions, applying some necessary limitation to the varying if the hrf is canonical\n",
    "        gen_params = np.zeros((size, 4))\n",
    "        \n",
    "        for set in range(len(gen_params)):\n",
    "            \n",
    "            # There's a better way to do this, for each set just generate a new parameter set instead of storing them all\n",
    "            # Fix this later\n",
    "            if canonical:\n",
    "                while gen_params[i][0] >= gen_params[i][1]:\n",
    "                    gen_params[i][0] = parameters[0] + np.random.normal(mean, var) # k1\n",
    "                    gen_params[i][1] = parameters[1] + np.random.normal(mean, var) # k2\n",
    "                while gen_params[i][2] >= gen_params[i][3]:\n",
    "                    gen_params[i][2] = parameters[2] + np.random.normal(mean, var) # theta1\n",
    "                    gen_params[i][3] = parameters[3] + np.random.normal(mean, var) # theta2\n",
    "            else:\n",
    "                gen_params[i][0] = parameters[0] + np.random.normal(mean, var) # k1\n",
    "                gen_params[i][1] = parameters[1] + np.random.normal(mean, var) # k2\n",
    "                gen_params[i][2] = parameters[2] + np.random.normal(mean, var) # theta1\n",
    "                gen_params[i][3] = parameters[3] + np.random.normal(mean, var) # theta2\n",
    "        \n",
    "        gamma_set = [[bio_modelling.bC_hrf(time_axis, gen_params[i][0], gen_params[i][1], gen_params[i][2], gen_params[i][3])] for i in range(size)]\n",
    "        return magnitude_correction * np.array(gamma_set)\n",
    "        \n",
    "    \n",
    "models = bio_modelling(random_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97ad24f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the length of gamma params 4\n",
      "this is the length of clean data 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/8vz112hj0ms1db0lb3xcvmch0000gn/T/ipykernel_96673/746220001.py:9: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  return  (t ** (k-1)) * (math.e ** (-t / theta)) / (gamma(k) * (theta** k))\n",
      "/var/folders/vt/8vz112hj0ms1db0lb3xcvmch0000gn/T/ipykernel_96673/746220001.py:9: RuntimeWarning: divide by zero encountered in divide\n",
      "  return  (t ** (k-1)) * (math.e ** (-t / theta)) / (gamma(k) * (theta** k))\n",
      "/var/folders/vt/8vz112hj0ms1db0lb3xcvmch0000gn/T/ipykernel_96673/746220001.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  return  (t ** (k-1)) * (math.e ** (-t / theta)) / (gamma(k) * (theta** k))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the length of noise data 2000\n",
      "This is the shape of the null_data (8000, 204)\n"
     ]
    }
   ],
   "source": [
    "# Actually generating the data\n",
    "signal_number = 10000\n",
    "signal_length = len(time_signal)\n",
    "initial_parameters = [3, 5, 2, 1]\n",
    "p_mean, p_var = 0, 0.01\n",
    "\n",
    "\n",
    "# Currently we'll be working on the first channel only, for simplicity's sake\n",
    "gamma_params = models.get_basic_gamma_params(avg_haemo_func_tapping.data[0], time_signal, initial_parameters)\n",
    "print(\"this is the length of gamma params\", len(gamma_params))\n",
    "clean_data = models.generate_gamma_set(time_signal, gamma_params, signal_number, magnitude_correction=1.2*1e-5)\n",
    "print(\"this is the length of clean data\", len(clean_data))\n",
    "noise_data = models.generate_noise_set(signal_number//5, signal_length)\n",
    "print(\"this is the length of noise data\", len(noise_data))\n",
    "null_data = np.zeros((signal_number - (signal_number//5),) + np.shape(noise_data)[1:])\n",
    "print(\"This is the shape of the null_data\", np.shape(null_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3272c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the length of the base_line data 10000\n"
     ]
    }
   ],
   "source": [
    "baseline_data = np.concatenate((noise_data, null_data))\n",
    "np.random.shuffle(baseline_data)\n",
    "print(\"This is the length of the base_line data\", len(baseline_data))\n",
    "overlayed_data = clean_data + baseline_data\n",
    "print(\"we done...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ee6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesssing the data\n",
    "z_overlay = convs.z_curve_normalisation(overlayed_data)\n",
    "z_clean = convs.z_curve_normalisation(overlayed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset classes for encoer\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Dataset for training with data as a set of amplitude points\n",
    "class amp_dataset:\n",
    "    def __init__(self, clean, overlayed, transform):\n",
    "        assert(len(clean) == len(overlayed))\n",
    "        self.clean = clean\n",
    "        self.overlayed = overlayed\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.overlayed)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "            \n",
    "        noisy = self.transform(overlayed[idx])\n",
    "        clean = self.transform(clean[idx])\n",
    "        \n",
    "        return noisy, clean\n",
    "    \n",
    "amp_set = amp_dataset(clean, overlayed_data, basic_transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amp_Lin_AutoEncoder(nn.Module):\n",
    "    # This is the linear neural network implementation of an auto-encoder using amplitude points\n",
    "    def __init__(self, signal_length):\n",
    "        # Signal length has to be longer than 200\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(signal_length, 124),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(124, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 54)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
